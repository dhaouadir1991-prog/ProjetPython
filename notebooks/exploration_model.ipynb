{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31011b95-c6c2-4723-8969-6e9126e23531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b249ad72-91ee-4b65-adfb-9f151cbbd143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"unitary/unbiased-toxic-roberta\",\n",
    "    top_k=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f194ff4-1c1e-4464-8921-a85c492ee9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity                  → 0.997\n",
      "insult                    → 0.996\n",
      "male                      → 0.013\n",
      "psychiatric_or_mental_illness → 0.007\n",
      "homosexual_gay_or_lesbian → 0.006\n",
      "female                    → 0.003\n",
      "identity_attack           → 0.003\n",
      "sexual_explicit           → 0.003\n",
      "christian                 → 0.002\n",
      "threat                    → 0.002\n",
      "obscene                   → 0.002\n",
      "black                     → 0.001\n",
      "jewish                    → 0.001\n",
      "muslim                    → 0.001\n",
      "white                     → 0.001\n",
      "severe_toxicity           → 0.000\n"
     ]
    }
   ],
   "source": [
    "text = \"You are a disgusting human being.\"\n",
    "scores = classifier(text)[0]\n",
    "\n",
    "for s in scores:\n",
    "    print(f\"{s['label']:25s} → {s['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2beb72-b8ac-4263-9b52-332ac03ef141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NON_ADEQUATE: True\n"
     ]
    }
   ],
   "source": [
    "NON_ADEQUATE = any(\n",
    "    s[\"score\"] > 0.7 and s[\"label\"] in [\n",
    "        \"toxicity\",\n",
    "        \"insult\",\n",
    "        \"threat\",\n",
    "        \"identity_attack\"\n",
    "    ]\n",
    "    for s in scores\n",
    ")\n",
    "\n",
    "print(\"NON_ADEQUATE:\", NON_ADEQUATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b175227-b159-4747-b50c-10f4d4186a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook exploration_model.ipynb to script\n",
      "[NbConvertApp] Writing 689 bytes to exploration_model.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script exploration_model.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers (PyTorch)",
   "language": "python",
   "name": "transformers_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
